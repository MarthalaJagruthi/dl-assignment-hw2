# -*- coding: utf-8 -*-
"""jagruthi-dl-assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VEun7o9f83jpLi0-caUraIhdPvjcrT0e
"""

!pip install -q -U transformers[torch] datasets evaluate rouge_score gradio accelerate
print("Installation complete.")
import os

import torch
from transformers import (
    pipeline,
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    DataCollatorForSeq2Seq
)
from datasets import load_dataset
import evaluate
import pandas as pd
import numpy as np
from tqdm.auto import tqdm
import gradio as gr
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Device: {device}")
if device == "cpu":
    print("Running on CPU")

print("Loading CNN/DailyMail dataset...")
dataset = load_dataset("cnn_dailymail", "3.0.0")
train_data = dataset['train'].shuffle(seed=42).select(range(1000))
test_data = dataset['test'].shuffle(seed=42).select(range(50))

metric = evaluate.load("rouge")

def evaluate_model(model_path, tokenizer_path=None):
    print(f"Evaluating {model_path}...")
    summarizer = pipeline("summarization", model=model_path, tokenizer=tokenizer_path or model_path, device=0 if device=="cuda" else -1)

    preds = []
    refs = []

    for i in tqdm(range(len(test_data))):
        text = test_data[i]['article'][:1024]
        try:
            res = summarizer(text, max_length=100, min_length=30, do_sample=False)
            preds.append(res[0]['summary_text'])
            refs.append(test_data[i]['highlights'])
        except:
            preds.append("")
            refs.append(test_data[i]['highlights'])

    scores = metric.compute(predictions=preds, references=refs)
    return scores

print("Baseline 1: T5-Small")
t5_base_scores = evaluate_model("t5-small")

print("Baseline 2: BART-Large")
bart_base_scores = evaluate_model("facebook/bart-large-cnn")

df_base = pd.DataFrame([t5_base_scores, bart_base_scores], index=["T5-Small (Base)", "BART-Large (Base)"])
print("\nBASELINE COMPARISON (Before Training):")
print(df_base[['rouge1', 'rouge2', 'rougeL']])

print("\nBASELINE COMPARISON (Before Training):")
print(df_base[['rouge1', 'rouge2', 'rougeL']])

def train_model(model_id, output_name):
    print(f"\nSTARTING TRAINING FOR: {model_id}")

    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_id)

    prefix = "summarize: " if "t5" in model_id else ""

    def preprocess_function(examples):
        inputs = [prefix + doc for doc in examples["article"]]
        model_inputs = tokenizer(inputs, max_length=512, truncation=True)
        labels = tokenizer(examples["highlights"], max_length=128, truncation=True)
        model_inputs["labels"] = labels["input_ids"]
        return model_inputs

    tokenized_train = train_data.map(preprocess_function, batched=True)
    tokenized_test = test_data.map(preprocess_function, batched=True)

    args = Seq2SeqTrainingArguments(
        output_dir=output_name,
        learning_rate=2e-5,
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        per_device_eval_batch_size=4,
        num_train_epochs=3,
        weight_decay=0.01,
        predict_with_generate=True,
        fp16=True if device=="cuda" else False,
        report_to="none",
        logging_steps=50,
        eval_strategy="steps",
        eval_steps=100,
        save_strategy="no"
    )

    trainer = Seq2SeqTrainer(
        model=model,
        args=args,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_test,
        tokenizer=tokenizer,
        data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model),
    )

    trainer.train()
    print(f"Finished training {model_id}")

    save_path = f"./fine_tuned_{output_name}"
    trainer.save_model(save_path)
    tokenizer.save_pretrained(save_path)
    return save_path

t5_save_path = train_model("t5-small", "t5_model")
bart_save_path = train_model("facebook/bart-large-cnn", "bart_model")

print("\nEVALUATING FINE-TUNED MODELS...")

t5_ft_scores = evaluate_model(t5_save_path)

bart_ft_scores = evaluate_model(bart_save_path)

all_results = [t5_base_scores, t5_ft_scores, bart_base_scores, bart_ft_scores]
indices = ["T5 (Base)", "T5 (Fine-Tuned)", "BART (Base)", "BART (Fine-Tuned)"]
df_leaderboard = pd.DataFrame(all_results, index=indices)

print("\nFINAL LEADERBOARD (Raw Scores):")
print(df_leaderboard[['rouge1', 'rouge2', 'rougeL']])

t5_delta = {k: t5_ft_scores[k] - t5_base_scores[k] for k in ['rouge1', 'rouge2', 'rougeL']}
bart_delta = {k: bart_ft_scores[k] - bart_base_scores[k] for k in ['rouge1', 'rouge2', 'rougeL']}

improvement_data = [
    {
        "Model": "T5",
        "Base ROUGE-1": t5_base_scores['rouge1'],
        "Fine-Tuned ROUGE-1": t5_ft_scores['rouge1'],
        "Improvement": t5_delta['rouge1']
    },
    {
        "Model": "BART",
        "Base ROUGE-1": bart_base_scores['rouge1'],
        "Fine-Tuned ROUGE-1": bart_ft_scores['rouge1'],
        "Improvement": bart_delta['rouge1']
    }
]

df_improvement = pd.DataFrame(improvement_data)
print("\nIMPROVEMENT ANALYSIS (Base vs Fine-Tuned):")
print(df_improvement.to_string(index=False))

import matplotlib.pyplot as plt

labels = ['ROUGE-1', 'ROUGE-2', 'ROUGE-L']
t5_base_vals = [t5_base_scores[k] for k in ['rouge1', 'rouge2', 'rougeL']]
t5_ft_vals = [t5_ft_scores[k] for k in ['rouge1', 'rouge2', 'rougeL']]
bart_base_vals = [bart_base_scores[k] for k in ['rouge1', 'rouge2', 'rougeL']]
bart_ft_vals = [bart_ft_scores[k] for k in ['rouge1', 'rouge2', 'rougeL']]

x = np.arange(len(labels))
width = 0.2

fig, ax = plt.subplots(figsize=(12, 6))

rects1 = ax.bar(x - 1.5*width, t5_base_vals, width, label='T5 Base', color='#ff9999')
rects2 = ax.bar(x - 0.5*width, t5_ft_vals, width, label='T5 Fine-Tuned', color='#cc0000')
rects3 = ax.bar(x + 0.5*width, bart_base_vals, width, label='BART Base', color='#99ccff')
rects4 = ax.bar(x + 1.5*width, bart_ft_vals, width, label='BART Fine-Tuned', color='#0000cc')

ax.set_ylabel('Score')
ax.set_title('Base vs Fine-Tuned Performance (T5 & BART)')
ax.set_xticks(x)
ax.set_xticklabels(labels)
ax.legend()
ax.grid(axis='y', linestyle='--', alpha=0.7)

plt.show()

pipe_t5 = pipeline("summarization", model=t5_save_path, device=0 if device=="cuda" else -1)
pipe_bart = pipeline("summarization", model=bart_save_path, device=0 if device=="cuda" else -1)

def summarize_ui(text, model_choice):
    if model_choice == "T5 (Fine-Tuned)":
        res = pipe_t5(text, max_length=150)[0]['summary_text']
    else:
        res = pipe_bart(text, max_length=150)[0]['summary_text']
    return res

demo = gr.Interface(
    fn=summarize_ui,
    inputs=[
        gr.Textbox(lines=10, label="Input Article"),
        gr.Radio(["T5 (Fine-Tuned)", "BART (Fine-Tuned)"], label="Choose Model", value="T5 (Fine-Tuned)")
    ],
    outputs=gr.Textbox(label="Generated Summary", lines=10),
    title="Dual-Model Summarizer",
    description="Compare the outputs of your two fine-tuned models."
)

demo.launch(share=True, debug=True)